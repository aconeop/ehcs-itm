{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 01:19:51.427284: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2023-06-22 01:20:08.032982: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow está utilizando la GPU:\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 23:26:58.480778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-21 23:26:58.558795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-21 23:26:58.559089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3\n",
      "coreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 1.93GiB deviceMemoryBandwidth: 194.55MiB/s\n",
      "2023-06-21 23:26:58.559410: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2023-06-21 23:26:58.819330: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-21 23:26:58.819652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-06-21 23:26:58.914992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-21 23:26:59.042074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-21 23:26:59.189638: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-21 23:26:59.295146: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-21 23:26:59.305287: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-06-21 23:26:59.305724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-21 23:26:59.306274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-21 23:26:59.306376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-06-21 23:26:59.414795: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "# Verificar los dispositivos disponibles\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "if len(devices) > 0:\n",
    "    print(\"TensorFlow está utilizando la GPU:\")\n",
    "    for device in devices:\n",
    "        print(device)\n",
    "else:\n",
    "    print(\"TensorFlow no está utilizando la GPU. Asegúrate de que los controladores de GPU estén instalados correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtcosenoF(par,fi,co):\n",
    "\n",
    "    # Coordenadas\n",
    "    [xfc, yfc] = np. meshgrid(np.linspace(1-co/2, co/2, co), np.linspace(fi/2, 1-fi/2, fi))\n",
    "\n",
    "    # Normalizar cooredenadas en intervalo (-pi, pi) y crear filtros en\n",
    "    # dirección horizontal y vertical\n",
    "    fc1 = np.cos(xfc*(math.pi/par)*(1/np.max(xfc)))**2\n",
    "    fc2 = np.cos(yfc*(math.pi/par)*(1/np.max(yfc)))**2\n",
    "\n",
    "    # Intersectar ambas direcciones\n",
    "    fc = (fc1 > 0)*fc1*(fc2 > 0)*fc2\n",
    "\n",
    "    # Re-escalar de 0 a 1\n",
    "    return fc/np.max(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo de ejecución: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Obtener información sobre el dispositivo de una operación o tensor\n",
    "device = c.device\n",
    "print(\"Dispositivo de ejecución:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpycuda\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdriver\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcuda\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Obtener el contexto de la GPU\u001b[39;00m\n\u001b[1;32m      5\u001b[0m gpu_device_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/job:localhost/replica:0/task:0/device:GPU:0\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycuda'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "# Obtener el contexto de la GPU\n",
    "gpu_device_name = '/job:localhost/replica:0/task:0/device:GPU:0'\n",
    "gpu_device = tf.config.experimental.get_device_details(gpu_device_name)\n",
    "\n",
    "# Obtener el identificador de la GPU\n",
    "gpu_id = int(gpu_device_name.split(':')[-1])\n",
    "\n",
    "# Inicializar el driver de CUDA\n",
    "cuda.init()\n",
    "\n",
    "# Obtener el contexto de la GPU\n",
    "cuda_device = cuda.Device(gpu_id)\n",
    "\n",
    "# Obtener las especificaciones de la GPU\n",
    "name = cuda_device.name()\n",
    "compute_capability = cuda_device.compute_capability()\n",
    "total_memory = cuda_device.total_memory()\n",
    "\n",
    "print(\"Especificaciones de la GPU:\")\n",
    "print(\"Nombre: \", name)\n",
    "print(\"Capacidad de cómputo: \", compute_capability)\n",
    "print(\"Memoria total: \", total_memory/(1024**2), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepairholoF(CH_m,dx,L):\n",
    "\n",
    "    [row, col] = CH_m.shape\n",
    "    Wx = dx * col\n",
    "    Wy = dx * row\n",
    "\n",
    "    # Matriz coordinates\n",
    "    [X, Y] = np.meshgrid(range(col), range(row))\n",
    "\n",
    "    # Hologram origin coordinates\n",
    "    xo = -Wx / 2\n",
    "    yo = -Wy / 2\n",
    "\n",
    "    # Prepared Hologram, coordinates origin\n",
    "    xop = xo * L / math.sqrt(L ** 2 + xo ** 2)\n",
    "    yop = yo * L / math.sqrt(L ** 2 + yo ** 2)\n",
    "\n",
    "    Xp = dx*(X-col/2)*L/np.sqrt(L**2+(dx**2)*(X-col/2)**2+(dx**2)*(Y-row/2)**2)\n",
    "    Yp = dx*(Y-row/2)*L/np.sqrt(L**2+(dx**2)*(X-col/2)**2+(dx**2)*(Y-row/2)**2)\n",
    "    # New coordinates measured in units of the -2*xop/(row) pixel size\n",
    "    Xcoord=(Xp-xop)/(-2*xop/col)\n",
    "    Ycoord=(Yp-yop)/(-2*xop/row)\n",
    "    \n",
    "    # Find lowest integer\n",
    "    iXcoord = np.floor(Xcoord).astype(np.uint16)\n",
    "    iYcoord = np.floor(Ycoord).astype(np.uint16)\n",
    "    \n",
    "    # Calculate the fractioning for interpolation\n",
    "    x1frac = (iXcoord + 1.0)-Xcoord                #upper value to integer\n",
    "    x2frac = 1.0-x1frac                            #lower value to integer\n",
    "    y1frac = (iYcoord + 1.0)-Ycoord\n",
    "    y2frac = 1.0-y1frac\n",
    "    \n",
    "    x1y1 = x1frac*y1frac                          #Corresponding pixel areas for each direction\n",
    "    x1y2 = x1frac*y2frac\n",
    "    x2y1 = x2frac*y1frac\n",
    "    x2y2 = x2frac*y2frac\n",
    "    # En éstas matrices estèn los factores de interpolación para la proyección del holograma en un sistema esférico\n",
    "    # estos facotores estan mapeados con las coordenandas en iCoord, buscar la manera de mapearlos a las coordenadas\n",
    "    # del holograma. (interpolación bilineal?)\n",
    "    \n",
    "    # Pre-allocate the prepared hologram\n",
    "    CHp_m = np.zeros((row, col))\n",
    "    \n",
    "    # Prepare hologram (preparation1 - every pixel remapping)\n",
    "    for it in range(row-1):\n",
    "        for jt in range(col-1):\n",
    "            CHp_m[iYcoord[it, jt], iXcoord[it, jt]] = CHp_m[iYcoord[it, jt], iXcoord[it, jt]] + \\\n",
    "                                                      x1y1[it, jt]*CH_m[it, jt]\n",
    "            CHp_m[iYcoord[it, jt], iXcoord[it, jt]+1] = CHp_m[iYcoord[it, jt], iXcoord[it, jt]+1] + \\\n",
    "                                                        x2y1[it, jt]*CH_m[it, jt]\n",
    "            CHp_m[iYcoord[it, jt]+1, iXcoord[it, jt]] = CHp_m[iYcoord[it, jt]+1, iXcoord[it, jt]] + \\\n",
    "                                                        x1y2[it, jt]*CH_m[it, jt]\n",
    "            CHp_m[iYcoord[it, jt]+1, iXcoord[it, jt]+1] = CHp_m[iYcoord[it, jt]+1, iXcoord[it, jt]+1] + \\\n",
    "                                                          x2y2[it, jt]*CH_m[it, jt]\n",
    "    return CHp_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepairholoF_remap(CH_m,dx,L):\n",
    "\n",
    "    [row, col] = CH_m.shape\n",
    "    Wx = dx * col\n",
    "    Wy = dx * row\n",
    "\n",
    "    # Matriz coordinates\n",
    "    [X, Y] = np.meshgrid(range(col), range(row))\n",
    "    X = X - col/2\n",
    "    Y = Y - row/2\n",
    "\n",
    "    r = np.sqrt(X**2 + Y**2)\n",
    "    rW = np.sqrt((Wx/2)**2 + (Wy/2)**2)\n",
    "    auxL = np.max(r)*L/np.max(rW)\n",
    "    theta = np.arctan2(Y, X)\n",
    "    R = auxL*np.sin(r/auxL)\n",
    "\n",
    "    Xcoord = R*np.cos(theta)\n",
    "    Ycoord = R*np.sin(theta)\n",
    "    Xcoord = (Xcoord+col/2).astype(np.float32)\n",
    "    Ycoord = (Ycoord+row/2).astype(np.float32)\n",
    "    CHp_m = cv2.remap(CH_m, Xcoord, Ycoord, cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "\n",
    "    return CHp_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kreuzer3F(CHp_m, z, L, lamb, deltaX, FC):\n",
    "\n",
    "    [row, col] = CHp_m.shape\n",
    "    Wx = dx * col\n",
    "    Wy = dx * row\n",
    "\n",
    "    # Hologram origin coordinates\n",
    "    xo = -Wx / 2\n",
    "    yo = -Wy / 2\n",
    "\n",
    "    # Prepared Hologram, coordinates origin\n",
    "    xop = xo * L / math.sqrt(L ** 2 + xo ** 2)\n",
    "    yop = yo * L / math.sqrt(L ** 2 + yo ** 2)\n",
    "\n",
    "    #np to TNP (TensorNumPy)\n",
    "    CHp_m = tnp.asarray(CHp_m)\n",
    "    z = tnp.asarray(z)\n",
    "    L = tnp.asarray(L)\n",
    "    xop = tnp.asarray(xop)\n",
    "    yop = tnp.asarray(yop)\n",
    "    deltaX = tnp.asarray(deltaX)\n",
    "\n",
    "    # Square pixels:\n",
    "    deltaY = deltaX\n",
    "\n",
    "    # Matrix size\n",
    "    [row, col] = CHp_m.shape\n",
    "\n",
    "    # Parameters\n",
    "    k = tnp.asarray(2*math.pi/lamb)\n",
    "\n",
    "    # Matriz coordinates\n",
    "    [X, Y] = tnp.meshgrid(range(col), range(row))\n",
    "    X = X.astype(tnp.float64)\n",
    "    Y = Y.astype(tnp.float64)\n",
    "    # Pixel size for the prepared hologram\n",
    "    deltaxp = xop / (-col/2)\n",
    "    deltayp = yop / (-row/2)\n",
    "\n",
    "    # Coordinates origin for the reconstruction plane\n",
    "    Yo = -deltaX * (row/2)\n",
    "    Xo = -deltaX * (col/2)\n",
    "\n",
    "    # Multiply prepared hologram with propagation phase\n",
    "    Rp = tnp.sqrt((L**2)-(deltaxp*X+xop)**2-(deltayp*Y+yop)**2)\n",
    "    r = tnp.sqrt((deltaX**2)*((X-col/2)**2+(Y-row/2)**2)+z**2)\n",
    "    CHp_m = CHp_m*((L/Rp)**4)*np.exp(-0.5j*k*(r**2-2*z*L)*Rp/(L**2))\n",
    "\n",
    "    #.* exp(0.125 * 1i * k * ((L ^ 2)./ Rp).* ((r.^ 2 - 2 * z * L).* (Rp./ (L ^ 2)).^ 2).^ 2);...\n",
    "    #.* exp(-(3 / 48) * 1i * k * ((L ^ 2)./ Rp).* ((r.^ 2 - 2 * z * L).* (Rp./ (L ^ 2)).^ 2).^ 3)...\n",
    "    #.* exp((15 / 384) * 1i * k * ((L ^ 2)./ Rp).* ((r.^ 2 - 2 * z * L).* (Rp./ (L ^ 2)).^ 2).^ 4);\n",
    "\n",
    "    # Padding constant value\n",
    "    pad = int(col/2)\n",
    "\n",
    "    # Padding on the cosine rowlter\n",
    "    FC = np.pad(FC, pad)\n",
    "\n",
    "    # Convolution operation\n",
    "    # First transform\n",
    "    T1 = CHp_m*tnp.exp((1j*k/(2*L))*(2*Xo*X*deltaxp+2*Yo*Y*deltayp+X**2*deltaxp*deltaX+Y**2*deltayp*deltaY))\n",
    "    T1 = np.pad(T1, pad)\n",
    "    T1 = tf.signal.fftshift(tf.signal.fft2d(tf.signal.fftshift(T1*FC)))\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Second transform\n",
    "    T2 = tnp.exp(-1j*(k/(2 * L)) * ((X-col/2)**2 * deltaxp * deltaX + (Y-row/2)**2 * deltayp * deltaY))\n",
    "    T2 = np.pad(T2, pad)\n",
    "    T2 = tf.signal.fftshift(tf.signal.fft2d(tf.signal.fftshift(T2*FC)))\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Third transform\n",
    "    K = tf.signal.ifftshift(tf.signal.ifft2d(tf.signal.ifftshift(T2*T1)))\n",
    "    tf.keras.backend.clear_session()\n",
    "    K = K[pad:(pad+row), pad:(pad+col)]\n",
    "\n",
    "    # Multiply by aditional terms after the propagation\n",
    "    # K = K * deltaxp * deltayp * (exp(sqrt(-1) * (k / L) * ((Xo + X * deltaX) * xop + (Yo + Y * deltaY) * yop)))...\n",
    "    # .*exp(sqrt(-1) * (0.5 * k/L) * ((X - 0 * col/2)**2 * deltaxp * deltaX + (Y - 0 * row / 2)**2 *deltayp*deltaY))\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'/home/jetson/Downloads/H05 (3).png'\n",
    "holo = np.array(Image.open(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "holo= cv2.resize(holo,(128,128),interpolation=cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fi, co,] = holo.shape\n",
    "holo = holo[:, int((co-fi)/2):co-int((co-fi)/2)]\n",
    "[fi, co] = holo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "def limpiar():\n",
    "    K.clear_session()  # limpiar la memoria\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "limpiar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "holoContrast = holo  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_files = listdir('.')\n",
    "stored_filt = list(filter(lambda x: 'filtcos.npy' in x, fold_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine filter loaded\n"
     ]
    }
   ],
   "source": [
    "if len(list(filter(lambda x: (str(co)+'-'+str(fi)) in x, fold_files))) == 0:\n",
    "    fc = filtcosenoF(100, fi, co)\n",
    "    np.save(str(co)+'-'+str(fi)+'_filtcos.npy', fc)\n",
    "    print('Cosine filter saved')\n",
    "else:\n",
    "    fc = np.load(str(co)+'-'+str(fi)+'_filtcos.npy')\n",
    "    print('Cosine filter loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "limpiar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 5e-3\n",
    "L = 8e-3\n",
    "lamb = 532e-9\n",
    "dx = 1.55e-6 #tamaño del pixel\n",
    "\n",
    "# LET'S FIND THE BEST FOCUSED RECONSTRUCTION\n",
    "# DLHM reconstruction\n",
    "# pixel size at reconstruction plane\n",
    "deltaX = z*dx/L  # in micrometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_holo = prepairholoF_remap(holoContrast, dx, L) # Faster (0.11 Seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 01:24:37.270758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-22 01:24:37.331571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:24:37.350936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3\n",
      "coreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 1.93GiB deviceMemoryBandwidth: 194.55MiB/s\n",
      "2023-06-22 01:24:37.351177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2023-06-22 01:24:37.551019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-22 01:24:37.551451: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-06-22 01:24:37.627454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-22 01:24:37.741064: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-22 01:24:37.867711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-22 01:24:37.951425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-22 01:24:37.962702: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-06-22 01:24:37.963168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:24:37.963700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:24:37.963823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-06-22 01:24:38.082618: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-06-22 01:24:38.169843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:24:38.170006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3\n",
      "coreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 1.93GiB deviceMemoryBandwidth: 194.55MiB/s\n",
      "2023-06-22 01:24:38.170120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2023-06-22 01:24:38.170218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-22 01:24:38.170286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-06-22 01:24:38.170379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-22 01:24:38.170447: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-22 01:24:38.170504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-22 01:24:38.170559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-22 01:24:38.170613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-06-22 01:24:38.170831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:24:38.171052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:24:38.171116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-06-22 01:25:54.656213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-22 01:25:54.786268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-06-22 01:25:54.790935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-06-22 01:25:57.964496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:25:58.708015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:25:58.742991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:908] ARM64 does not support NUMA - returning NUMA node zero\n",
      "2023-06-22 01:25:59.047697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 46 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3)\n",
      "2023-06-22 01:25:59.295343: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-06-22 01:26:16.594477: W tensorflow/core/common_runtime/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.25MiB (rounded to 8652800)requested by op Pow\n",
      "Current allocation summary follows.\n",
      "2023-06-22 01:26:16.610376: I tensorflow/core/common_runtime/bfc_allocator.cc:972] BFCAllocator dump for GPU_0_bfc\n",
      "2023-06-22 01:26:16.630218: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (256): \tTotal Chunks: 12, Chunks in use: 12. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 96B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630304: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630334: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630358: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630384: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0. 5.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630409: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 8.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630433: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630457: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630479: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630500: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630522: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630543: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630566: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630589: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630612: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630640: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (8388608): \tTotal Chunks: 5, Chunks in use: 5. 46.46MiB allocated for chunks. 46.46MiB in use in bin. 41.26MiB client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630665: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630688: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630713: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630746: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630771: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-06-22 01:26:16.630802: I tensorflow/core/common_runtime/bfc_allocator.cc:995] Bin for 8.25MiB was 8.00MiB, Chunk State: \n",
      "2023-06-22 01:26:16.630827: I tensorflow/core/common_runtime/bfc_allocator.cc:1008] Next region of size 48734208\n",
      "2023-06-22 01:26:16.651716: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50000 of size 256 next 7\n",
      "2023-06-22 01:26:16.651794: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50100 of size 256 next 8\n",
      "2023-06-22 01:26:16.651818: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50200 of size 256 next 9\n",
      "2023-06-22 01:26:16.651839: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50300 of size 256 next 10\n",
      "2023-06-22 01:26:16.651860: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50400 of size 256 next 11\n",
      "2023-06-22 01:26:16.651879: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50500 of size 256 next 12\n",
      "2023-06-22 01:26:16.651898: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50600 of size 256 next 13\n",
      "2023-06-22 01:26:16.651917: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50700 of size 256 next 14\n",
      "2023-06-22 01:26:16.651936: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50800 of size 256 next 15\n",
      "2023-06-22 01:26:16.651954: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50900 of size 256 next 16\n",
      "2023-06-22 01:26:16.651973: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a50a00 of size 256 next 19\n",
      "2023-06-22 01:26:16.651992: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at f00a50b00 of size 5632 next 1\n",
      "2023-06-22 01:26:16.652019: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a52100 of size 1280 next 2\n",
      "2023-06-22 01:26:16.652040: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at f00a52600 of size 8448 next 3\n",
      "2023-06-22 01:26:16.652059: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a54700 of size 256 next 4\n",
      "2023-06-22 01:26:16.652080: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f00a54800 of size 8652800 next 5\n",
      "2023-06-22 01:26:16.652100: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f01295000 of size 8652800 next 6\n",
      "2023-06-22 01:26:16.652119: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f01ad5800 of size 8652800 next 18\n",
      "2023-06-22 01:26:16.652138: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f02316000 of size 8652800 next 17\n",
      "2023-06-22 01:26:16.652158: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at f02b56800 of size 14104576 next 18446744073709551615\n",
      "2023-06-22 01:26:16.652177: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]      Summary of in-use Chunks by size: \n",
      "2023-06-22 01:26:16.652207: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 12 Chunks of size 256 totalling 3.0KiB\n",
      "2023-06-22 01:26:16.652231: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-06-22 01:26:16.652255: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 4 Chunks of size 8652800 totalling 33.01MiB\n",
      "2023-06-22 01:26:16.652277: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 14104576 totalling 13.45MiB\n",
      "2023-06-22 01:26:16.652298: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Sum Total of in-use chunks: 46.46MiB\n",
      "2023-06-22 01:26:16.652323: I tensorflow/core/common_runtime/bfc_allocator.cc:1042] total_region_allocated_bytes_: 48734208 memory_limit_: 48734208 available bytes: 0 curr_region_allocation_bytes_: 97468416\n",
      "2023-06-22 01:26:16.652361: I tensorflow/core/common_runtime/bfc_allocator.cc:1048] Stats: \n",
      "Limit:                        48734208\n",
      "InUse:                        48720128\n",
      "MaxInUse:                     48720128\n",
      "NumAllocs:                          35\n",
      "MaxAllocSize:                 14104576\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-06-22 01:26:16.652386: W tensorflow/core/common_runtime/bfc_allocator.cc:441] *****************************************************************************************xxxxxxxxxxx\n",
      "2023-06-22 01:26:16.843674: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cwise_ops_common.h:128 : Resource exhausted: OOM when allocating tensor with shape[1040,1040] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1040,1040] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Pow]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reconstruction \u001b[39m=\u001b[39m kreuzer3F(pre_holo, z, L, lamb, deltaX, fc)\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mkreuzer3F\u001b[0;34m(CHp_m, z, L, lamb, deltaX, FC)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m# Multiply prepared hologram with propagation phase\u001b[39;00m\n\u001b[1;32m     45\u001b[0m Rp \u001b[39m=\u001b[39m tnp\u001b[39m.\u001b[39msqrt((L\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m-\u001b[39m(deltaxp\u001b[39m*\u001b[39mX\u001b[39m+\u001b[39mxop)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m-\u001b[39m(deltayp\u001b[39m*\u001b[39mY\u001b[39m+\u001b[39myop)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m r \u001b[39m=\u001b[39m tnp\u001b[39m.\u001b[39msqrt((deltaX\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m*\u001b[39m((X\u001b[39m-\u001b[39mcol\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m+\u001b[39m(Y\u001b[39m-\u001b[39;49mrow\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m)\u001b[39m+\u001b[39mz\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     47\u001b[0m CHp_m \u001b[39m=\u001b[39m CHp_m\u001b[39m*\u001b[39m((L\u001b[39m/\u001b[39mRp)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39mj\u001b[39m*\u001b[39mk\u001b[39m*\u001b[39m(r\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mz\u001b[39m*\u001b[39mL)\u001b[39m*\u001b[39mRp\u001b[39m/\u001b[39m(L\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[1;32m     49\u001b[0m \u001b[39m#.* exp(0.125 * 1i * k * ((L ^ 2)./ Rp).* ((r.^ 2 - 2 * z * L).* (Rp./ (L ^ 2)).^ 2).^ 2);...\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m#.* exp(-(3 / 48) * 1i * k * ((L ^ 2)./ Rp).* ((r.^ 2 - 2 * z * L).* (Rp./ (L ^ 2)).^ 2).^ 3)...\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m#.* exp((15 / 384) * 1i * k * ((L ^ 2)./ Rp).* ((r.^ 2 - 2 * z * L).* (Rp./ (L ^ 2)).^ 2).^ 4);\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[39m# Padding constant value\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/numpy_ops/np_math_ops.py:957\u001b[0m, in \u001b[0;36m_wrap.<locals>._f\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(b, \u001b[39m'\u001b[39m\u001b[39m__array_priority__\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    954\u001b[0m            \u001b[39m0\u001b[39m) \u001b[39m>\u001b[39m np_arrays\u001b[39m.\u001b[39mndarray\u001b[39m.\u001b[39m__array_priority__:\n\u001b[1;32m    955\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[39mreturn\u001b[39;00m f(a, b)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/numpy_ops/np_math_ops.py:350\u001b[0m, in \u001b[0;36mpower\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39m@np_utils\u001b[39m\u001b[39m.\u001b[39mnp_doc(\u001b[39m'\u001b[39m\u001b[39mpower\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    349\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpower\u001b[39m(x1, x2):\n\u001b[0;32m--> 350\u001b[0m   \u001b[39mreturn\u001b[39;00m _bin_op(math_ops\u001b[39m.\u001b[39;49mpow, x1, x2)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/numpy_ops/np_math_ops.py:77\u001b[0m, in \u001b[0;36m_bin_op\u001b[0;34m(tf_fun, a, b, promote)\u001b[0m\n\u001b[1;32m     75\u001b[0m   a \u001b[39m=\u001b[39m np_array_ops\u001b[39m.\u001b[39marray(a)\n\u001b[1;32m     76\u001b[0m   b \u001b[39m=\u001b[39m np_array_ops\u001b[39m.\u001b[39marray(b)\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m np_utils\u001b[39m.\u001b[39mtensor_to_ndarray(tf_fun(a\u001b[39m.\u001b[39;49mdata, b\u001b[39m.\u001b[39;49mdata))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py:670\u001b[0m, in \u001b[0;36mpow\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes the power of one value to another.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \n\u001b[1;32m    650\u001b[0m \u001b[39mGiven a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39m  A `Tensor`.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(name, \u001b[39m\"\u001b[39m\u001b[39mPow\u001b[39m\u001b[39m\"\u001b[39m, [x]) \u001b[39mas\u001b[39;00m name:\n\u001b[0;32m--> 670\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49m_pow(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py:6554\u001b[0m, in \u001b[0;36m_pow\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6552\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6553\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 6554\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   6555\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   6556\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6862\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m message \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6861\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 6862\u001b[0m six\u001b[39m.\u001b[39;49mraise_from(core\u001b[39m.\u001b[39;49m_status_to_exception(e\u001b[39m.\u001b[39;49mcode, message), \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1040,1040] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Pow]"
     ]
    }
   ],
   "source": [
    "reconstruction = kreuzer3F(pre_holo, z, L, lamb, deltaX, fc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
